# Model Configuration
model:
  name: "MoK_CNN_19"
  architecture: "SpatialProcessor"  # Example architecture
  spatial_input_shape: [1440, 481]  # 36 channels: 15 surface + 18 pressure + 1 land_sea_mask + 2 coords
  target: 1
  num_classes: 9  # Number of classes for classification (0-8 for binned onset dates)
  dropout_rate: 0.2  # Dropout probability for fc1, fc2, fc3, and fc4 layers

  # Activation Function Configuration
  activation:
    type: "prelu"  # Options: "relu", "prelu", "leaky_relu"
    # PReLU options (only used when type="prelu")
    prelu_mode: "channel"  # Options: "shared" (1 param), "channel" (1 param per channel)
    # LeakyReLU options (only used when type="leaky_relu")
    leaky_relu_slope: 0.01  # Negative slope for LeakyReLU (typical: 0.01 to 0.2)

  # Weight Initialization
  weight_init:
    enabled: true  # If false, use PyTorch's default initialization
    method: "kaiming_normal"  # Options: kaiming_normal, kaiming_uniform, xavier_normal, xavier_uniform, orthogonal, normal, uniform, constant, zeros, ones, default
    activation: "relu"  # Activation function type (relu, leaky_relu, tanh, sigmoid, elu, gelu)
    bias_init: "zeros"  # Bias initialization (zeros, constant, normal, uniform)
    bias_value: 0.0  # Value for constant bias init
    mode: "fan_out"  # Mode for kaiming init (fan_in or fan_out)
    gain: 1.0  # Gain for xavier/orthogonal init
    verbose: true  # Print initialization details

# Training Configuration
training:
  epochs: 50 # batch size is set in data section
  learning_rate: 0.01
  optimizer: "adamw"
  loss: "cross_entropy"  # CrossEntropyLoss for multiclass classification
  random_seed: 42  # For reproducibility
  weight_decay: 0.0001  # L2 regularization strength (typical values: 1e-4 to 1e-5)

  # Learning Rate Scheduler
  # Options: "ReduceLROnPlateau", "StepLR", "OneCycleLR", "ExponentialLR"
  lr_scheduler:
    enabled: true
    type: "OneCycleLR"  # Scheduler type

    # ReduceLROnPlateau: Reduce LR when metric plateaus
    mode: "min"  # 'min' for loss, 'max' for accuracy/R2
    factor: 0.5  # Multiply LR by this factor when reducing (new_lr = lr * factor)
    patience: 5  # Number of epochs with no improvement before reducing LR
    min_lr: 1.0e-6  # Minimum learning rate
    threshold: 1.0e-4  # Threshold for measuring improvement
    restore_best_on_reduce: true  # If true, restore model/optimizer to best epoch when LR is reduced

    # StepLR: Step decay every N epochs
    # Example config:
    #   type: "StepLR"
    #   step_size: 30  # Reduce LR every 30 epochs
    #   gamma: 0.1     # LR *= 0.1 at each step
    # ExponentialLR: Exponential decay every epoch
    # Example config:
    #   type: "ExponentialLR"
    #   gamma: 0.95  # LR *= 0.95 every epoch (lr_epoch = lr_initial × gamma^epoch)
    step_size: 30
    gamma: 0.1

    # OneCycleLR: Cyclical learning rate with warmup and annealing
    # Example config:
    #   type: "OneCycleLR"
    #   max_lr: 0.1              # Peak learning rate
    #   pct_start: 0.3           # Percent of cycle spent increasing LR (warmup)
    #   anneal_strategy: "cos"   # 'cos' or 'linear'
    #   div_factor: 25.0         # initial_lr = max_lr / div_factor
    #   final_div_factor: 1e4    # final_lr = max_lr / (div_factor * final_div_factor)
    max_lr: 0.1
    pct_start: 0.3
    anneal_strategy: "cos"
    div_factor: 25.0
    final_div_factor: 10000.0

    

  # Callbacks
  early_stopping:
    enabled: false
    patience: 10
    monitor: "val_loss"

  checkpoint:
    enabled: true
    save_best_only: true
    monitor: "val_loss"

# Data Configuration
data:
  # Path to ERA5 monthly NetCDF files
  data_dir: "/gdata2/ERA5/monthly"

  # Path to target CSV file (Year, DateRelJun01, OnsetBin, OnsetBinCode)
  # For classification: use MoKDates_binned.csv with OnsetBinCode (0-8)
  target_file: "/home/deepakns/Work/data/MoKDates_binned.csv"

  # Input variables to use from the data
  # Surface variables: sst, ttr, tcc, t2m, msl
  input_geo_var_surf: [sst]
  # Pressure level variables: u, v, z
  input_geo_var_press: []

  # Optional static channels (set to false to exclude)
  include_lat: false
  include_lon: false
  include_landsea: false

  # Year specifications can be:
  # - Single years: 1962, 1965
  # - Ranges: "1950:1960" (inclusive, both start and end)
  # - Mix of both: ["1950:1960", 1962, "1964:2000"]
  train_years: ["1951:2000"]
  val_years: ["2001:2010"]
  test_years: ["2011:2024"]

  # DataLoader settings
  batch_size: 1
  num_workers: 2

  # Dataset settings
  # Time step indices to load per year (each year produces ONE sample)
  # For example, time_steps: [0, 1, 2] will load the first 3 months (e.g., Feb, Mar, Apr)
  # Non-consecutive steps are allowed: [0, 2, 4] will load months 1, 3, 5
  # Each time step is stacked as a separate channel
  time_steps: [1, 2]
  pressure_levels: [0, 1]  # Indices of pressure levels to extract

  # Normalization strategy
  # 0: No normalization (raw data)
  # 1: Normalize using training data statistics (spatially-varying mean/std)
  # Future: Additional strategies can be added
  normalize_strategy: 1

  augmentation:
    enabled: true
    rotation_range: 15
    width_shift_range: 0.1
    height_shift_range: 0.1
    horizontal_flip: true
    # Gaussian noise augmentation (applied during training only)
    gaussian_noise:
      enabled: true
      std: 0.01  # Standard deviation of noise (relative to normalized data, typical: 0.01-0.1)

# Logging
logging:
  log_dir: "logs"
  tensorboard: true  # Keep TensorBoard enabled
  save_frequency: 1

  # Weights & Biases Configuration - ENABLED
  wandb:
    enabled: true  # ← Set to true to enable W&B

    # Basic Settings
    project: "MoK_date_predict"  # W&B project name
    entity: null  # Your W&B username or team (null = use default)
    name: null  # Run name (null = auto-generate from model name)

    # Organization
    tags:  # Tags for organizing experiments
      - "with-scheduler"
      - "lr-0.01"
      - "batch_size-1"
      - "dropout-0.2"
      - "initial-kaiming-normal"
      - "coarsen-to4x4"
    notes: "Training with OneCycleLR"  # Experiment notes

    # Logging Options
    log_model: true  # Save best model checkpoint to W&B
    log_predictions: false  # Don't log prediction tables (saves time/space)
    log_gradients: false  # Don't log gradient histograms (saves time)

    # Advanced Options
    watch_model: false  # Don't use wandb.watch() (can slow training)
    watch_log: "gradients"  # What to log if watch_model is true
    watch_freq: 100  # How often to log if watch_model is true