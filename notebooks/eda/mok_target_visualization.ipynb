{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MoK Target Variable Visualization\n",
    "\n",
    "This notebook provides comprehensive visualizations and statistical analysis of the MoK (Monsoon Onset over Kerala) target variable: **DateRelJun01** (days relative to June 1st).\n",
    "\n",
    "## Dataset Information\n",
    "- **Target Variable**: DateRelJun01 - Days relative to June 1st\n",
    "- **Time Range**: 1940-2025\n",
    "- **Negative values**: Monsoon onset before June 1st\n",
    "- **Positive values**: Monsoon onset after June 1st\n",
    "- **Zero**: Monsoon onset exactly on June 1st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MoK dates data\n",
    "data_path = '/home/deepakns/Work/data/MoKDates.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Display basic info\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nData types:\\n{df.dtypes}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DateRelJun01 to actual dates for better interpretation\n",
    "def rel_days_to_date(year, days_rel):\n",
    "    \"\"\"Convert days relative to June 1st to actual date\"\"\"\n",
    "    june_1 = datetime(year, 6, 1)\n",
    "    actual_date = june_1 + timedelta(days=days_rel)\n",
    "    return actual_date\n",
    "\n",
    "df['ActualDate'] = df.apply(lambda row: rel_days_to_date(int(row['Year']), row['DateRelJun01']), axis=1)\n",
    "df['Month'] = df['ActualDate'].dt.month\n",
    "df['Day'] = df['ActualDate'].dt.day\n",
    "df['MonthName'] = df['ActualDate'].dt.strftime('%B')\n",
    "df['DateString'] = df['ActualDate'].dt.strftime('%B %d')\n",
    "\n",
    "print(\"Sample data with converted dates:\")\n",
    "df[['Year', 'DateRelJun01', 'ActualDate', 'DateString']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "print(\"=\"*60)\n",
    "print(\"DESCRIPTIVE STATISTICS FOR DateRelJun01\")\n",
    "print(\"=\"*60)\n",
    "print(df['DateRelJun01'].describe())\n",
    "print(f\"\\nSkewness: {df['DateRelJun01'].skew():.4f}\")\n",
    "print(f\"Kurtosis: {df['DateRelJun01'].kurtosis():.4f}\")\n",
    "print(f\"\\nMode: {df['DateRelJun01'].mode().values}\")\n",
    "print(f\"Median: {df['DateRelJun01'].median():.2f}\")\n",
    "print(f\"Mean: {df['DateRelJun01'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of onset timing\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ONSET TIMING DISTRIBUTION\")\n",
    "print(\"=\"*60)\n",
    "early_onset = (df['DateRelJun01'] < 0).sum()\n",
    "on_time = (df['DateRelJun01'] == 0).sum()\n",
    "late_onset = (df['DateRelJun01'] > 0).sum()\n",
    "\n",
    "print(f\"Early onset (before June 1): {early_onset} ({early_onset/len(df)*100:.1f}%)\")\n",
    "print(f\"On-time onset (June 1): {on_time} ({on_time/len(df)*100:.1f}%)\")\n",
    "print(f\"Late onset (after June 1): {late_onset} ({late_onset/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nEarliest onset: {df['DateRelJun01'].min():.0f} days (Year: {df.loc[df['DateRelJun01'].idxmin(), 'Year']:.0f}) - {df.loc[df['DateRelJun01'].idxmin(), 'DateString']}\")\n",
    "print(f\"Latest onset: {df['DateRelJun01'].max():.0f} days (Year: {df.loc[df['DateRelJun01'].idxmax(), 'Year']:.0f}) - {df.loc[df['DateRelJun01'].idxmax(), 'DateString']}\")\n",
    "print(f\"Range: {df['DateRelJun01'].max() - df['DateRelJun01'].min():.0f} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Time Series Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series plot with train/val/test splits\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "# Define splits based on config\n",
    "train_mask = (df['Year'] >= 1951) & (df['Year'] <= 2000)\n",
    "val_mask = (df['Year'] >= 2001) & (df['Year'] <= 2010)\n",
    "test_mask = (df['Year'] >= 2011) & (df['Year'] <= 2024)\n",
    "\n",
    "# Plot data\n",
    "ax.plot(df[train_mask]['Year'], df[train_mask]['DateRelJun01'], 'o-', label='Train (1951-2000)', alpha=0.7, linewidth=2)\n",
    "ax.plot(df[val_mask]['Year'], df[val_mask]['DateRelJun01'], 's-', label='Validation (2001-2010)', alpha=0.7, linewidth=2)\n",
    "ax.plot(df[test_mask]['Year'], df[test_mask]['DateRelJun01'], '^-', label='Test (2011-2024)', alpha=0.7, linewidth=2)\n",
    "\n",
    "# Add reference lines\n",
    "ax.axhline(y=0, color='red', linestyle='--', alpha=0.5, label='June 1st')\n",
    "ax.axhline(y=df['DateRelJun01'].mean(), color='green', linestyle='--', alpha=0.5, label=f'Mean ({df[\"DateRelJun01\"].mean():.1f} days)')\n",
    "\n",
    "# Styling\n",
    "ax.set_xlabel('Year', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Days Relative to June 1st', fontsize=12, fontweight='bold')\n",
    "ax.set_title('MoK Onset Date Time Series (1940-2025)', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='best', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series with rolling statistics\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "# Calculate rolling statistics\n",
    "window = 10\n",
    "rolling_mean = df['DateRelJun01'].rolling(window=window, center=True).mean()\n",
    "rolling_std = df['DateRelJun01'].rolling(window=window, center=True).std()\n",
    "\n",
    "# Plot\n",
    "ax.plot(df['Year'], df['DateRelJun01'], 'o-', alpha=0.4, label='Actual', markersize=4)\n",
    "ax.plot(df['Year'], rolling_mean, 'r-', linewidth=2.5, label=f'{window}-year Rolling Mean')\n",
    "ax.fill_between(df['Year'], \n",
    "                rolling_mean - rolling_std, \n",
    "                rolling_mean + rolling_std, \n",
    "                alpha=0.2, label=f'{window}-year Rolling Std Dev')\n",
    "\n",
    "ax.axhline(y=0, color='black', linestyle='--', alpha=0.5, label='June 1st')\n",
    "\n",
    "ax.set_xlabel('Year', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Days Relative to June 1st', fontsize=12, fontweight='bold')\n",
    "ax.set_title(f'MoK Onset with {window}-Year Rolling Statistics', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='best', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Histogram with KDE\n",
    "axes[0, 0].hist(df['DateRelJun01'], bins=30, edgecolor='black', alpha=0.7, density=True)\n",
    "df['DateRelJun01'].plot(kind='kde', ax=axes[0, 0], linewidth=2, color='red')\n",
    "axes[0, 0].axvline(df['DateRelJun01'].mean(), color='green', linestyle='--', linewidth=2, label=f'Mean: {df[\"DateRelJun01\"].mean():.1f}')\n",
    "axes[0, 0].axvline(df['DateRelJun01'].median(), color='orange', linestyle='--', linewidth=2, label=f'Median: {df[\"DateRelJun01\"].median():.1f}')\n",
    "axes[0, 0].set_xlabel('Days Relative to June 1st', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Density', fontweight='bold')\n",
    "axes[0, 0].set_title('Distribution with KDE', fontweight='bold', fontsize=12)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "bp = axes[0, 1].boxplot(df['DateRelJun01'], vert=True, patch_artist=True,\n",
    "                        boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
    "                        medianprops=dict(color='red', linewidth=2),\n",
    "                        whiskerprops=dict(linewidth=1.5),\n",
    "                        capprops=dict(linewidth=1.5))\n",
    "axes[0, 1].set_ylabel('Days Relative to June 1st', fontweight='bold')\n",
    "axes[0, 1].set_title('Box Plot', fontweight='bold', fontsize=12)\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add statistics to box plot\n",
    "stats_text = f\"Min: {df['DateRelJun01'].min():.0f}\\nQ1: {df['DateRelJun01'].quantile(0.25):.0f}\\nMedian: {df['DateRelJun01'].median():.0f}\\nQ3: {df['DateRelJun01'].quantile(0.75):.0f}\\nMax: {df['DateRelJun01'].max():.0f}\"\n",
    "axes[0, 1].text(1.3, df['DateRelJun01'].median(), stats_text, fontsize=9, verticalalignment='center')\n",
    "\n",
    "# Violin plot\n",
    "parts = axes[1, 0].violinplot([df['DateRelJun01']], positions=[0], showmeans=True, showmedians=True)\n",
    "axes[1, 0].set_ylabel('Days Relative to June 1st', fontweight='bold')\n",
    "axes[1, 0].set_title('Violin Plot', fontweight='bold', fontsize=12)\n",
    "axes[1, 0].set_xticks([0])\n",
    "axes[1, 0].set_xticklabels(['DateRelJun01'])\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Q-Q plot for normality check\n",
    "stats.probplot(df['DateRelJun01'], dist=\"norm\", plot=axes[1, 1])\n",
    "axes[1, 1].set_title('Q-Q Plot (Normality Test)', fontweight='bold', fontsize=12)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normality test\n",
    "from scipy.stats import shapiro, normaltest\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"NORMALITY TESTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Shapiro-Wilk test\n",
    "shapiro_stat, shapiro_p = shapiro(df['DateRelJun01'])\n",
    "print(f\"\\nShapiro-Wilk Test:\")\n",
    "print(f\"  Test Statistic: {shapiro_stat:.4f}\")\n",
    "print(f\"  P-value: {shapiro_p:.4f}\")\n",
    "print(f\"  Result: {'Normal distribution' if shapiro_p > 0.05 else 'Not normal distribution'} (α=0.05)\")\n",
    "\n",
    "# D'Agostino's K-squared test\n",
    "normal_stat, normal_p = normaltest(df['DateRelJun01'])\n",
    "print(f\"\\nD'Agostino's K-squared Test:\")\n",
    "print(f\"  Test Statistic: {normal_stat:.4f}\")\n",
    "print(f\"  P-value: {normal_p:.4f}\")\n",
    "print(f\"  Result: {'Normal distribution' if normal_p > 0.05 else 'Not normal distribution'} (α=0.05)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Frequency and Categorical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Month-wise distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Count by month\n",
    "month_counts = df['MonthName'].value_counts().sort_index()\n",
    "month_order = ['May', 'June', 'July']\n",
    "month_counts = month_counts.reindex(month_order, fill_value=0)\n",
    "\n",
    "axes[0].bar(month_counts.index, month_counts.values, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Month', fontweight='bold')\n",
    "axes[0].set_ylabel('Frequency', fontweight='bold')\n",
    "axes[0].set_title('Onset Frequency by Month', fontweight='bold', fontsize=12)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add counts on bars\n",
    "for i, (month, count) in enumerate(month_counts.items()):\n",
    "    axes[0].text(i, count + 0.5, str(int(count)), ha='center', fontweight='bold')\n",
    "\n",
    "# Onset category pie chart\n",
    "categories = ['Early\\n(Before Jun 1)', 'On-time\\n(Jun 1)', 'Late\\n(After Jun 1)']\n",
    "values = [early_onset, on_time, late_onset]\n",
    "colors = ['#ff9999', '#66b3ff', '#99ff99']\n",
    "explode = (0.05, 0.05, 0.05)\n",
    "\n",
    "axes[1].pie(values, labels=categories, autopct='%1.1f%%', startangle=90, \n",
    "            colors=colors, explode=explode, textprops={'fontsize': 11, 'fontweight': 'bold'})\n",
    "axes[1].set_title('Onset Timing Categories', fontweight='bold', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value counts and frequency table\n",
    "value_counts = df['DateRelJun01'].value_counts().sort_index()\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MOST COMMON ONSET DATES\")\n",
    "print(\"=\"*60)\n",
    "top_dates = df['DateRelJun01'].value_counts().head(10)\n",
    "for days, count in top_dates.items():\n",
    "    date_str = df[df['DateRelJun01'] == days]['DateString'].iloc[0]\n",
    "    print(f\"  {days:3.0f} days ({date_str:15s}): {count} occurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Temporal Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decade-wise analysis\n",
    "df['Decade'] = (df['Year'] // 10) * 10\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 12))\n",
    "\n",
    "# Box plot by decade\n",
    "decade_data = [df[df['Decade'] == d]['DateRelJun01'].values for d in sorted(df['Decade'].unique())]\n",
    "decade_labels = [f\"{int(d)}s\" for d in sorted(df['Decade'].unique())]\n",
    "\n",
    "bp = axes[0].boxplot(decade_data, labels=decade_labels, patch_artist=True)\n",
    "for patch in bp['boxes']:\n",
    "    patch.set_facecolor('lightblue')\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "axes[0].axhline(y=0, color='red', linestyle='--', alpha=0.5, label='June 1st')\n",
    "axes[0].set_xlabel('Decade', fontweight='bold')\n",
    "axes[0].set_ylabel('Days Relative to June 1st', fontweight='bold')\n",
    "axes[0].set_title('Onset Date Distribution by Decade', fontweight='bold', fontsize=12)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Bar plot of decade means\n",
    "decade_stats = df.groupby('Decade')['DateRelJun01'].agg(['mean', 'std']).reset_index()\n",
    "x_pos = range(len(decade_stats))\n",
    "\n",
    "axes[1].bar(x_pos, decade_stats['mean'], yerr=decade_stats['std'], \n",
    "            capsize=5, alpha=0.7, edgecolor='black')\n",
    "axes[1].axhline(y=0, color='red', linestyle='--', alpha=0.5, label='June 1st')\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels(decade_labels)\n",
    "axes[1].set_xlabel('Decade', fontweight='bold')\n",
    "axes[1].set_ylabel('Mean Days Relative to June 1st', fontweight='bold')\n",
    "axes[1].set_title('Mean Onset Date by Decade (with Std Dev)', fontweight='bold', fontsize=12)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decade-wise statistics table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DECADE-WISE STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "decade_summary = df.groupby('Decade')['DateRelJun01'].agg([\n",
    "    ('Count', 'count'),\n",
    "    ('Mean', 'mean'),\n",
    "    ('Median', 'median'),\n",
    "    ('Std Dev', 'std'),\n",
    "    ('Min', 'min'),\n",
    "    ('Max', 'max')\n",
    "]).round(2)\n",
    "decade_summary.index = [f\"{int(d)}s\" for d in decade_summary.index]\n",
    "print(decade_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Trend Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear trend analysis\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# Perform linear regression\n",
    "slope, intercept, r_value, p_value, std_err = linregress(df['Year'], df['DateRelJun01'])\n",
    "\n",
    "# Generate trend line\n",
    "trend_line = slope * df['Year'] + intercept\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "ax.scatter(df['Year'], df['DateRelJun01'], alpha=0.6, s=50, label='Actual Data')\n",
    "ax.plot(df['Year'], trend_line, 'r-', linewidth=2.5, label=f'Linear Trend (slope={slope:.4f})')\n",
    "ax.axhline(y=0, color='green', linestyle='--', alpha=0.5, label='June 1st')\n",
    "\n",
    "# Add equation and statistics\n",
    "equation_text = f\"y = {slope:.4f}x + {intercept:.2f}\\nR² = {r_value**2:.4f}\\np-value = {p_value:.4f}\"\n",
    "ax.text(0.02, 0.98, equation_text, transform=ax.transAxes, \n",
    "        fontsize=11, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "ax.set_xlabel('Year', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Days Relative to June 1st', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Linear Trend Analysis of MoK Onset Dates', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper right', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print interpretation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TREND ANALYSIS RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Slope: {slope:.6f} days/year\")\n",
    "print(f\"R-squared: {r_value**2:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "print(f\"Standard Error: {std_err:.6f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    trend_direction = \"delayed\" if slope > 0 else \"earlier\"\n",
    "    print(f\"\\n✓ Significant trend detected: Monsoon onset is becoming {trend_direction} over time\")\n",
    "else:\n",
    "    print(f\"\\n✗ No significant trend detected (p > 0.05)\")\n",
    "\n",
    "# Calculate change over the period\n",
    "years_span = df['Year'].max() - df['Year'].min()\n",
    "total_change = slope * years_span\n",
    "print(f\"\\nEstimated change from {df['Year'].min():.0f} to {df['Year'].max():.0f}: {total_change:.2f} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Autocorrelation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autocorrelation and Partial Autocorrelation\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
    "\n",
    "# Autocorrelation Function (ACF)\n",
    "plot_acf(df['DateRelJun01'], lags=20, ax=axes[0], alpha=0.05)\n",
    "axes[0].set_title('Autocorrelation Function (ACF)', fontweight='bold', fontsize=12)\n",
    "axes[0].set_xlabel('Lag', fontweight='bold')\n",
    "axes[0].set_ylabel('Autocorrelation', fontweight='bold')\n",
    "\n",
    "# Partial Autocorrelation Function (PACF)\n",
    "plot_pacf(df['DateRelJun01'], lags=20, ax=axes[1], alpha=0.05)\n",
    "axes[1].set_title('Partial Autocorrelation Function (PACF)', fontweight='bold', fontsize=12)\n",
    "axes[1].set_xlabel('Lag', fontweight='bold')\n",
    "axes[1].set_ylabel('Partial Autocorrelation', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Split-wise Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare statistics across train/val/test splits\n",
    "df['Split'] = 'Other'\n",
    "df.loc[train_mask, 'Split'] = 'Train'\n",
    "df.loc[val_mask, 'Split'] = 'Validation'\n",
    "df.loc[test_mask, 'Split'] = 'Test'\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Box plot by split\n",
    "split_order = ['Train', 'Validation', 'Test']\n",
    "split_data = [df[df['Split'] == s]['DateRelJun01'].values for s in split_order]\n",
    "\n",
    "bp = axes[0].boxplot(split_data, labels=split_order, patch_artist=True)\n",
    "colors = ['#ff9999', '#66b3ff', '#99ff99']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "axes[0].axhline(y=0, color='red', linestyle='--', alpha=0.5, label='June 1st')\n",
    "axes[0].set_ylabel('Days Relative to June 1st', fontweight='bold')\n",
    "axes[0].set_title('Distribution by Data Split', fontweight='bold', fontsize=12)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Bar plot of means\n",
    "split_stats = df[df['Split'].isin(split_order)].groupby('Split')['DateRelJun01'].agg(['mean', 'std'])\n",
    "split_stats = split_stats.reindex(split_order)\n",
    "\n",
    "x_pos = range(len(split_stats))\n",
    "axes[1].bar(x_pos, split_stats['mean'], yerr=split_stats['std'], \n",
    "            capsize=5, alpha=0.7, edgecolor='black', color=colors)\n",
    "axes[1].axhline(y=0, color='red', linestyle='--', alpha=0.5, label='June 1st')\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels(split_order)\n",
    "axes[1].set_ylabel('Mean Days Relative to June 1st', fontweight='bold')\n",
    "axes[1].set_title('Mean Onset Date by Split (with Std Dev)', fontweight='bold', fontsize=12)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for i, (mean, std) in enumerate(zip(split_stats['mean'], split_stats['std'])):\n",
    "    axes[1].text(i, mean + std + 0.5, f'{mean:.1f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split-wise statistics table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SPLIT-WISE STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "split_summary = df[df['Split'].isin(split_order)].groupby('Split')['DateRelJun01'].agg([\n",
    "    ('Count', 'count'),\n",
    "    ('Mean', 'mean'),\n",
    "    ('Median', 'median'),\n",
    "    ('Std Dev', 'std'),\n",
    "    ('Min', 'min'),\n",
    "    ('Max', 'max'),\n",
    "    ('Range', lambda x: x.max() - x.min())\n",
    "]).round(2)\n",
    "split_summary = split_summary.reindex(split_order)\n",
    "print(split_summary)\n",
    "\n",
    "# Check for distribution shift\n",
    "from scipy.stats import ks_2samp\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DISTRIBUTION SHIFT TESTS (Kolmogorov-Smirnov)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "train_data = df[train_mask]['DateRelJun01']\n",
    "val_data = df[val_mask]['DateRelJun01']\n",
    "test_data = df[test_mask]['DateRelJun01']\n",
    "\n",
    "ks_stat_tv, ks_p_tv = ks_2samp(train_data, val_data)\n",
    "ks_stat_tt, ks_p_tt = ks_2samp(train_data, test_data)\n",
    "\n",
    "print(f\"\\nTrain vs Validation:\")\n",
    "print(f\"  KS Statistic: {ks_stat_tv:.4f}\")\n",
    "print(f\"  P-value: {ks_p_tv:.4f}\")\n",
    "print(f\"  Result: {'Significantly different' if ks_p_tv < 0.05 else 'Not significantly different'} (α=0.05)\")\n",
    "\n",
    "print(f\"\\nTrain vs Test:\")\n",
    "print(f\"  KS Statistic: {ks_stat_tt:.4f}\")\n",
    "print(f\"  P-value: {ks_p_tt:.4f}\")\n",
    "print(f\"  Result: {'Significantly different' if ks_p_tt < 0.05 else 'Not significantly different'} (α=0.05)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Outlier Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify outliers using IQR method\n",
    "Q1 = df['DateRelJun01'].quantile(0.25)\n",
    "Q3 = df['DateRelJun01'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = df[(df['DateRelJun01'] < lower_bound) | (df['DateRelJun01'] > upper_bound)]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"OUTLIER ANALYSIS (IQR Method)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Q1: {Q1:.2f}\")\n",
    "print(f\"Q3: {Q3:.2f}\")\n",
    "print(f\"IQR: {IQR:.2f}\")\n",
    "print(f\"Lower Bound: {lower_bound:.2f}\")\n",
    "print(f\"Upper Bound: {upper_bound:.2f}\")\n",
    "print(f\"\\nNumber of outliers: {len(outliers)} ({len(outliers)/len(df)*100:.1f}%)\")\n",
    "\n",
    "if len(outliers) > 0:\n",
    "    print(f\"\\nOutlier years and values:\")\n",
    "    for _, row in outliers.sort_values('DateRelJun01').iterrows():\n",
    "        print(f\"  {row['Year']:.0f}: {row['DateRelJun01']:6.0f} days ({row['DateString']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outliers\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "# Plot all data\n",
    "ax.scatter(df['Year'], df['DateRelJun01'], alpha=0.5, s=50, label='Normal')\n",
    "\n",
    "# Highlight outliers\n",
    "if len(outliers) > 0:\n",
    "    ax.scatter(outliers['Year'], outliers['DateRelJun01'], \n",
    "              color='red', s=100, marker='X', label='Outliers', zorder=5)\n",
    "    \n",
    "    # Annotate outliers\n",
    "    for _, row in outliers.iterrows():\n",
    "        ax.annotate(f\"{row['Year']:.0f}\\n{row['DateRelJun01']:.0f}d\", \n",
    "                   xy=(row['Year'], row['DateRelJun01']),\n",
    "                   xytext=(10, 10), textcoords='offset points',\n",
    "                   fontsize=8, bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7),\n",
    "                   arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "\n",
    "# Add bounds\n",
    "ax.axhline(y=lower_bound, color='orange', linestyle='--', alpha=0.5, label=f'Lower Bound ({lower_bound:.1f})')\n",
    "ax.axhline(y=upper_bound, color='orange', linestyle='--', alpha=0.5, label=f'Upper Bound ({upper_bound:.1f})')\n",
    "ax.axhline(y=0, color='green', linestyle='--', alpha=0.5, label='June 1st')\n",
    "\n",
    "ax.set_xlabel('Year', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Days Relative to June 1st', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Outlier Detection in MoK Onset Dates', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='best', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary and Key Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SUMMARY OF KEY INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1. DATA OVERVIEW\")\n",
    "print(f\"   • Total observations: {len(df)}\")\n",
    "print(f\"   • Time span: {df['Year'].min():.0f} - {df['Year'].max():.0f}\")\n",
    "print(f\"   • Date range: {df['DateString'].iloc[df['DateRelJun01'].idxmin()]} to {df['DateString'].iloc[df['DateRelJun01'].idxmax()]}\")\n",
    "\n",
    "print(f\"\\n2. CENTRAL TENDENCY\")\n",
    "print(f\"   • Mean: {df['DateRelJun01'].mean():.2f} days relative to June 1st\")\n",
    "print(f\"   • Median: {df['DateRelJun01'].median():.2f} days\")\n",
    "print(f\"   • Mode: {df['DateRelJun01'].mode().values[0]:.0f} days\")\n",
    "\n",
    "print(f\"\\n3. VARIABILITY\")\n",
    "print(f\"   • Standard deviation: {df['DateRelJun01'].std():.2f} days\")\n",
    "print(f\"   • Range: {df['DateRelJun01'].max() - df['DateRelJun01'].min():.0f} days\")\n",
    "print(f\"   • IQR: {IQR:.2f} days\")\n",
    "\n",
    "print(f\"\\n4. DISTRIBUTION SHAPE\")\n",
    "print(f\"   • Skewness: {df['DateRelJun01'].skew():.4f} ({'right-skewed' if df['DateRelJun01'].skew() > 0 else 'left-skewed'})\")\n",
    "print(f\"   • Kurtosis: {df['DateRelJun01'].kurtosis():.4f} ({'leptokurtic (heavy tails)' if df['DateRelJun01'].kurtosis() > 0 else 'platykurtic (light tails)'})\")\n",
    "print(f\"   • Normality: {'Approximately normal' if shapiro_p > 0.05 else 'Not normal'} (Shapiro-Wilk p={shapiro_p:.4f})\")\n",
    "\n",
    "print(f\"\\n5. TEMPORAL TRENDS\")\n",
    "trend_direction = \"delayed\" if slope > 0 else \"earlier\"\n",
    "trend_significance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "print(f\"   • Linear trend: {slope:.6f} days/year ({trend_direction})\")\n",
    "print(f\"   • Trend significance: {trend_significance} (p={p_value:.4f})\")\n",
    "print(f\"   • Total change ({df['Year'].min():.0f}-{df['Year'].max():.0f}): {total_change:.2f} days\")\n",
    "\n",
    "print(f\"\\n6. ONSET TIMING CATEGORIES\")\n",
    "print(f\"   • Early (before Jun 1): {early_onset} years ({early_onset/len(df)*100:.1f}%)\")\n",
    "print(f\"   • On-time (Jun 1): {on_time} years ({on_time/len(df)*100:.1f}%)\")\n",
    "print(f\"   • Late (after Jun 1): {late_onset} years ({late_onset/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n7. OUTLIERS\")\n",
    "print(f\"   • Number of outliers: {len(outliers)} ({len(outliers)/len(df)*100:.1f}%)\")\n",
    "if len(outliers) > 0:\n",
    "    extreme_early = outliers.loc[outliers['DateRelJun01'].idxmin()]\n",
    "    extreme_late = outliers.loc[outliers['DateRelJun01'].idxmax()]\n",
    "    print(f\"   • Most extreme early: {extreme_early['Year']:.0f} ({extreme_early['DateRelJun01']:.0f} days, {extreme_early['DateString']})\")\n",
    "    print(f\"   • Most extreme late: {extreme_late['Year']:.0f} ({extreme_late['DateRelJun01']:.0f} days, {extreme_late['DateString']})\")\n",
    "\n",
    "print(f\"\\n8. DATA SPLITS\")\n",
    "for split in split_order:\n",
    "    split_df = df[df['Split'] == split]\n",
    "    print(f\"   • {split}: {len(split_df)} samples, mean={split_df['DateRelJun01'].mean():.2f}d, std={split_df['DateRelJun01'].std():.2f}d\")\n",
    "\n",
    "print(f\"\\n9. DISTRIBUTION SHIFT\")\n",
    "print(f\"   • Train vs Validation: {'Significantly different' if ks_p_tv < 0.05 else 'Similar'} (KS p={ks_p_tv:.4f})\")\n",
    "print(f\"   • Train vs Test: {'Significantly different' if ks_p_tt < 0.05 else 'Similar'} (KS p={ks_p_tt:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "This notebook provides a comprehensive analysis of the MoK onset dates from 1940-2025. Key findings:\n",
    "\n",
    "1. **Target Variable Range**: The onset dates vary significantly, with extremes from May to July\n",
    "2. **Distribution**: The data shows specific characteristics in terms of skewness and kurtosis\n",
    "3. **Temporal Patterns**: Analysis reveals any trends or patterns over the decades\n",
    "4. **Data Quality**: Outlier analysis helps identify unusual years\n",
    "5. **Model Implications**: Split-wise analysis ensures fair train/validation/test distribution\n",
    "\n",
    "These visualizations and statistics provide valuable context for:\n",
    "- Model training and evaluation\n",
    "- Understanding prediction challenges\n",
    "- Identifying potential biases or shifts in the data\n",
    "- Setting realistic performance expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 12. Binned Analysis of Onset Dates\n\nIn this section, we create meaningful bins for onset dates to better understand the distribution patterns:\n- **On or Before May 20**: Very early onset (≤ May 20)\n- **May 21-24**: Early onset\n- **May 25-27**: Moderately early\n- **May 28-30**: Slightly early\n- **May 31-June 2**: Normal/on-time\n- **June 3-5**: Slightly late\n- **June 6-8**: Moderately late\n- **June 9-12**: Late onset\n- **After June 12**: Very late onset",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Define bins based on days relative to June 1st\n# May 20 = -12 days, May 21 = -11 days, May 24 = -8 days, May 25 = -7 days\n# May 27 = -5 days, May 28 = -4 days, May 30 = -2 days, May 31 = -1 day\n# June 1 = 0 days, June 2 = +1 day, June 3 = +2 days, June 5 = +4 days\n# June 6 = +5 days, June 8 = +7 days, June 9 = +8 days, June 12 = +11 days\n\n# Using pandas.cut with right=True (default): bins are (lower, upper]\n# So we need edges that create: ≤-12, (-12,-8], (-8,-5], (-5,-2], (-2,+1], (+1,+4], (+4,+7], (+7,+11], >+11\nbin_edges = [-np.inf, -12, -8, -5, -2, 1, 4, 7, 11, np.inf]\nbin_labels = [\n    'On/Before May 20\\n(≤ -12d)',\n    'May 21-24\\n(-12 to -9d)',\n    'May 25-27\\n(-8 to -6d)',\n    'May 28-30\\n(-5 to -3d)',\n    'May 31-Jun 2\\n(-2 to +1d)',\n    'June 3-5\\n(+2 to +4d)',\n    'June 6-8\\n(+5 to +7d)',\n    'June 9-12\\n(+8 to +11d)',\n    'After June 12\\n(> +11d)'\n]\n\n# Create binned categories\ndf['OnsetBin'] = pd.cut(df['DateRelJun01'], bins=bin_edges, labels=bin_labels, include_lowest=True)\n\n# Display bin distribution\nprint(\"=\"*80)\nprint(\"BINNED ONSET DATE DISTRIBUTION\")\nprint(\"=\"*80)\nbin_counts = df['OnsetBin'].value_counts().sort_index()\nprint(\"\\nFrequency by bin:\")\nfor bin_label, count in bin_counts.items():\n    percentage = count / len(df) * 100\n    print(f\"  {bin_label:25s}: {count:3d} occurrences ({percentage:5.1f}%)\")\n\nprint(f\"\\nTotal: {len(df)} observations\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Overall histogram of binned data\nfig, ax = plt.subplots(figsize=(16, 8))\n\nbin_counts_ordered = df['OnsetBin'].value_counts().reindex(bin_labels, fill_value=0)\n\n# Create color gradient\ncolors_palette = plt.cm.RdYlGn_r(np.linspace(0.1, 0.9, len(bin_labels)))\n\nbars = ax.bar(range(len(bin_labels)), bin_counts_ordered.values, \n               color=colors_palette, edgecolor='black', linewidth=1.5, alpha=0.8)\n\n# Add value labels on bars\nfor i, (bar, count) in enumerate(zip(bars, bin_counts_ordered.values)):\n    height = bar.get_height()\n    percentage = count / len(df) * 100\n    ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n            f'{int(count)}\\n({percentage:.1f}%)',\n            ha='center', va='bottom', fontweight='bold', fontsize=10)\n\nax.set_xticks(range(len(bin_labels)))\nax.set_xticklabels(bin_labels, rotation=45, ha='right', fontsize=10)\nax.set_xlabel('Onset Date Bins', fontsize=12, fontweight='bold')\nax.set_ylabel('Frequency', fontsize=12, fontweight='bold')\nax.set_title('Distribution of MoK Onset Dates by Bins (All Years)', fontsize=14, fontweight='bold')\nax.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 12.1 Binned Distribution by Decade",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Binned distribution by decade\ndecades = sorted(df['Decade'].unique())\ndecade_labels_list = [f\"{int(d)}s\" for d in decades]\n\n# Create a cross-tabulation\ndecade_bin_crosstab = pd.crosstab(df['Decade'], df['OnsetBin'])\ndecade_bin_crosstab = decade_bin_crosstab.reindex(columns=bin_labels, fill_value=0)\ndecade_bin_crosstab.index = decade_labels_list\n\nprint(\"=\"*100)\nprint(\"ONSET DATE BIN DISTRIBUTION BY DECADE (Count)\")\nprint(\"=\"*100)\nprint(decade_bin_crosstab)\n\n# Percentage within each decade\ndecade_bin_pct = pd.crosstab(df['Decade'], df['OnsetBin'], normalize='index') * 100\ndecade_bin_pct = decade_bin_pct.reindex(columns=bin_labels, fill_value=0)\ndecade_bin_pct.index = decade_labels_list\n\nprint(\"\\n\" + \"=\"*100)\nprint(\"ONSET DATE BIN DISTRIBUTION BY DECADE (Percentage)\")\nprint(\"=\"*100)\nprint(decade_bin_pct.round(1))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Stacked bar chart by decade\nfig, axes = plt.subplots(2, 1, figsize=(18, 14))\n\n# Prepare data for plotting\nx_pos = np.arange(len(decade_labels_list))\nwidth = 0.7\n\n# Plot 1: Stacked bar chart (counts)\nbottom = np.zeros(len(decade_labels_list))\nfor i, bin_label in enumerate(bin_labels):\n    values = decade_bin_crosstab[bin_label].values\n    bars = axes[0].bar(x_pos, values, width, bottom=bottom, \n                       label=bin_label.split('\\n')[0], \n                       color=colors_palette[i], edgecolor='black', linewidth=0.5)\n    bottom += values\n\naxes[0].set_xlabel('Decade', fontsize=12, fontweight='bold')\naxes[0].set_ylabel('Count', fontsize=12, fontweight='bold')\naxes[0].set_title('Onset Date Bin Distribution by Decade (Stacked Counts)', fontsize=14, fontweight='bold')\naxes[0].set_xticks(x_pos)\naxes[0].set_xticklabels(decade_labels_list)\naxes[0].legend(loc='upper left', bbox_to_anchor=(1.01, 1), fontsize=9)\naxes[0].grid(True, alpha=0.3, axis='y')\n\n# Plot 2: Stacked bar chart (percentages)\nbottom = np.zeros(len(decade_labels_list))\nfor i, bin_label in enumerate(bin_labels):\n    values = decade_bin_pct[bin_label].values\n    bars = axes[1].bar(x_pos, values, width, bottom=bottom,\n                       label=bin_label.split('\\n')[0],\n                       color=colors_palette[i], edgecolor='black', linewidth=0.5)\n    \n    # Add percentage labels for significant values\n    for j, (bar, val) in enumerate(zip(bars, values)):\n        if val > 5:  # Only label if > 5%\n            axes[1].text(bar.get_x() + bar.get_width()/2., \n                        bottom[j] + val/2,\n                        f'{val:.0f}%',\n                        ha='center', va='center', fontsize=8, fontweight='bold')\n    \n    bottom += values\n\naxes[1].set_xlabel('Decade', fontsize=12, fontweight='bold')\naxes[1].set_ylabel('Percentage (%)', fontsize=12, fontweight='bold')\naxes[1].set_title('Onset Date Bin Distribution by Decade (Stacked Percentages)', fontsize=14, fontweight='bold')\naxes[1].set_xticks(x_pos)\naxes[1].set_xticklabels(decade_labels_list)\naxes[1].legend(loc='upper left', bbox_to_anchor=(1.01, 1), fontsize=9)\naxes[1].grid(True, alpha=0.3, axis='y')\naxes[1].set_ylim([0, 100])\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Grouped bar chart by decade (better for comparison across bins)\nfig, ax = plt.subplots(figsize=(18, 8))\n\nn_bins = len(bin_labels)\nn_decades = len(decade_labels_list)\nwidth = 0.8 / n_bins\nx = np.arange(n_decades)\n\nfor i, bin_label in enumerate(bin_labels):\n    offset = (i - n_bins/2) * width + width/2\n    values = decade_bin_crosstab[bin_label].values\n    bars = ax.bar(x + offset, values, width, label=bin_label.split('\\n')[0], \n                  color=colors_palette[i], edgecolor='black', linewidth=0.5, alpha=0.9)\n\nax.set_xlabel('Decade', fontsize=12, fontweight='bold')\nax.set_ylabel('Count', fontsize=12, fontweight='bold')\nax.set_title('Onset Date Bin Distribution by Decade (Grouped Bars)', fontsize=14, fontweight='bold')\nax.set_xticks(x)\nax.set_xticklabels(decade_labels_list)\nax.legend(loc='upper left', bbox_to_anchor=(1.01, 1), fontsize=9)\nax.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 12.2 Binned Distribution by Data Split (Train/Validation/Test)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Binned distribution by data split\nsplit_order = ['Train', 'Validation', 'Test']\n\n# Create cross-tabulation\nsplit_bin_crosstab = pd.crosstab(df['Split'], df['OnsetBin'])\nsplit_bin_crosstab = split_bin_crosstab.reindex(index=split_order, columns=bin_labels, fill_value=0)\n\nprint(\"=\"*100)\nprint(\"ONSET DATE BIN DISTRIBUTION BY DATA SPLIT (Count)\")\nprint(\"=\"*100)\nprint(split_bin_crosstab)\n\n# Percentage within each split\nsplit_bin_pct = pd.crosstab(df['Split'], df['OnsetBin'], normalize='index') * 100\nsplit_bin_pct = split_bin_pct.reindex(index=split_order, columns=bin_labels, fill_value=0)\n\nprint(\"\\n\" + \"=\"*100)\nprint(\"ONSET DATE BIN DISTRIBUTION BY DATA SPLIT (Percentage)\")\nprint(\"=\"*100)\nprint(split_bin_pct.round(1))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualize split distributions\nfig, axes = plt.subplots(2, 2, figsize=(20, 12))\n\nsplit_colors = ['#ff9999', '#66b3ff', '#99ff99']\n\n# Plot 1: Stacked bar chart by split (counts)\nx_pos = np.arange(len(split_order))\nwidth = 0.6\n\nbottom = np.zeros(len(split_order))\nfor i, bin_label in enumerate(bin_labels):\n    values = split_bin_crosstab[bin_label].values\n    bars = axes[0, 0].bar(x_pos, values, width, bottom=bottom,\n                          label=bin_label.split('\\n')[0],\n                          color=colors_palette[i], edgecolor='black', linewidth=0.5)\n    bottom += values\n\naxes[0, 0].set_xlabel('Data Split', fontsize=12, fontweight='bold')\naxes[0, 0].set_ylabel('Count', fontsize=12, fontweight='bold')\naxes[0, 0].set_title('Onset Bin Distribution by Split (Stacked Counts)', fontsize=13, fontweight='bold')\naxes[0, 0].set_xticks(x_pos)\naxes[0, 0].set_xticklabels(split_order)\naxes[0, 0].legend(loc='upper left', bbox_to_anchor=(1.01, 1), fontsize=8)\naxes[0, 0].grid(True, alpha=0.3, axis='y')\n\n# Plot 2: Stacked bar chart by split (percentages)\nbottom = np.zeros(len(split_order))\nfor i, bin_label in enumerate(bin_labels):\n    values = split_bin_pct[bin_label].values\n    bars = axes[0, 1].bar(x_pos, values, width, bottom=bottom,\n                          label=bin_label.split('\\n')[0],\n                          color=colors_palette[i], edgecolor='black', linewidth=0.5)\n    \n    # Add percentage labels\n    for j, (bar, val) in enumerate(zip(bars, values)):\n        if val > 5:\n            axes[0, 1].text(bar.get_x() + bar.get_width()/2.,\n                           bottom[j] + val/2,\n                           f'{val:.0f}%',\n                           ha='center', va='center', fontsize=9, fontweight='bold')\n    \n    bottom += values\n\naxes[0, 1].set_xlabel('Data Split', fontsize=12, fontweight='bold')\naxes[0, 1].set_ylabel('Percentage (%)', fontsize=12, fontweight='bold')\naxes[0, 1].set_title('Onset Bin Distribution by Split (Stacked %)', fontsize=13, fontweight='bold')\naxes[0, 1].set_xticks(x_pos)\naxes[0, 1].set_xticklabels(split_order)\naxes[0, 1].legend(loc='upper left', bbox_to_anchor=(1.01, 1), fontsize=8)\naxes[0, 1].grid(True, alpha=0.3, axis='y')\naxes[0, 1].set_ylim([0, 100])\n\n# Plot 3: Grouped bar chart by split\nn_bins = len(bin_labels)\nn_splits = len(split_order)\nwidth_grouped = 0.7 / n_bins\nx = np.arange(n_splits)\n\nfor i, bin_label in enumerate(bin_labels):\n    offset = (i - n_bins/2) * width_grouped + width_grouped/2\n    values = split_bin_crosstab[bin_label].values\n    bars = axes[1, 0].bar(x + offset, values, width_grouped,\n                          label=bin_label.split('\\n')[0],\n                          color=colors_palette[i], edgecolor='black', linewidth=0.5, alpha=0.9)\n\naxes[1, 0].set_xlabel('Data Split', fontsize=12, fontweight='bold')\naxes[1, 0].set_ylabel('Count', fontsize=12, fontweight='bold')\naxes[1, 0].set_title('Onset Bin Distribution by Split (Grouped Bars)', fontsize=13, fontweight='bold')\naxes[1, 0].set_xticks(x)\naxes[1, 0].set_xticklabels(split_order)\naxes[1, 0].legend(loc='upper left', bbox_to_anchor=(1.01, 1), fontsize=8)\naxes[1, 0].grid(True, alpha=0.3, axis='y')\n\n# Plot 4: Side-by-side comparison for each split (separate histograms)\nfor idx, split in enumerate(split_order):\n    split_data = df[df['Split'] == split]['OnsetBin'].value_counts().reindex(bin_labels, fill_value=0)\n    x_plot = np.arange(len(bin_labels))\n    axes[1, 1].bar(x_plot + idx*0.25, split_data.values, width=0.24,\n                   label=split, color=split_colors[idx], edgecolor='black',\n                   linewidth=0.5, alpha=0.8)\n\naxes[1, 1].set_xlabel('Onset Date Bins', fontsize=12, fontweight='bold')\naxes[1, 1].set_ylabel('Count', fontsize=12, fontweight='bold')\naxes[1, 1].set_title('Onset Bin Comparison Across Splits', fontsize=13, fontweight='bold')\naxes[1, 1].set_xticks(x_plot + 0.25)\naxes[1, 1].set_xticklabels([label.split('\\n')[0] for label in bin_labels], rotation=45, ha='right', fontsize=8)\naxes[1, 1].legend(fontsize=10)\naxes[1, 1].grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 12.3 Heatmap Visualizations",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Heatmap visualizations\nfig, axes = plt.subplots(2, 1, figsize=(18, 14))\n\n# Heatmap 1: Decade vs Bins (counts)\nsns.heatmap(decade_bin_crosstab, annot=True, fmt='d', cmap='YlOrRd', \n            cbar_kws={'label': 'Count'}, linewidths=0.5, linecolor='gray',\n            ax=axes[0])\naxes[0].set_xlabel('Onset Date Bins', fontsize=12, fontweight='bold')\naxes[0].set_ylabel('Decade', fontsize=12, fontweight='bold')\naxes[0].set_title('Heatmap: Onset Bin Frequency by Decade (Count)', fontsize=14, fontweight='bold')\naxes[0].set_xticklabels([label.split('\\n')[0] for label in bin_labels], rotation=45, ha='right')\n\n# Heatmap 2: Decade vs Bins (percentages)\nsns.heatmap(decade_bin_pct, annot=True, fmt='.1f', cmap='YlOrRd',\n            cbar_kws={'label': 'Percentage (%)'}, linewidths=0.5, linecolor='gray',\n            ax=axes[1], vmin=0, vmax=100)\naxes[1].set_xlabel('Onset Date Bins', fontsize=12, fontweight='bold')\naxes[1].set_ylabel('Decade', fontsize=12, fontweight='bold')\naxes[1].set_title('Heatmap: Onset Bin Frequency by Decade (Percentage)', fontsize=14, fontweight='bold')\naxes[1].set_xticklabels([label.split('\\n')[0] for label in bin_labels], rotation=45, ha='right')\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Heatmap for data splits\nfig, axes = plt.subplots(2, 1, figsize=(18, 8))\n\n# Heatmap 1: Split vs Bins (counts)\nsns.heatmap(split_bin_crosstab, annot=True, fmt='d', cmap='RdYlGn_r',\n            cbar_kws={'label': 'Count'}, linewidths=0.5, linecolor='gray',\n            ax=axes[0])\naxes[0].set_xlabel('Onset Date Bins', fontsize=12, fontweight='bold')\naxes[0].set_ylabel('Data Split', fontsize=12, fontweight='bold')\naxes[0].set_title('Heatmap: Onset Bin Frequency by Data Split (Count)', fontsize=14, fontweight='bold')\naxes[0].set_xticklabels([label.split('\\n')[0] for label in bin_labels], rotation=45, ha='right')\n\n# Heatmap 2: Split vs Bins (percentages)\nsns.heatmap(split_bin_pct, annot=True, fmt='.1f', cmap='RdYlGn_r',\n            cbar_kws={'label': 'Percentage (%)'}, linewidths=0.5, linecolor='gray',\n            ax=axes[1], vmin=0, vmax=100)\naxes[1].set_xlabel('Onset Date Bins', fontsize=12, fontweight='bold')\naxes[1].set_ylabel('Data Split', fontsize=12, fontweight='bold')\naxes[1].set_title('Heatmap: Onset Bin Frequency by Data Split (Percentage)', fontsize=14, fontweight='bold')\naxes[1].set_xticklabels([label.split('\\n')[0] for label in bin_labels], rotation=45, ha='right')\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 12.4 Statistical Analysis of Binned Data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Statistical tests for bin distribution across splits\nfrom scipy.stats import chi2_contingency\n\nprint(\"=\"*80)\nprint(\"CHI-SQUARE TEST FOR INDEPENDENCE\")\nprint(\"=\"*80)\n\n# Test 1: Split vs Bins\nprint(\"\\n1. Testing if bin distribution is independent of data split:\")\nprint(\"   H0: Bin distribution is independent of data split\")\nprint(\"   H1: Bin distribution depends on data split\")\n\nchi2_split, p_split, dof_split, expected_split = chi2_contingency(split_bin_crosstab)\n\nprint(f\"\\n   Chi-square statistic: {chi2_split:.4f}\")\nprint(f\"   Degrees of freedom: {dof_split}\")\nprint(f\"   P-value: {p_split:.6f}\")\nprint(f\"   Result: {'REJECT H0' if p_split < 0.05 else 'FAIL TO REJECT H0'} (α=0.05)\")\n\nif p_split < 0.05:\n    print(\"   → Bin distributions are significantly different across splits\")\nelse:\n    print(\"   → Bin distributions are similar across splits\")\n\n# Test 2: Decade vs Bins\nprint(\"\\n2. Testing if bin distribution is independent of decade:\")\nprint(\"   H0: Bin distribution is independent of decade\")\nprint(\"   H1: Bin distribution depends on decade\")\n\nchi2_decade, p_decade, dof_decade, expected_decade = chi2_contingency(decade_bin_crosstab)\n\nprint(f\"\\n   Chi-square statistic: {chi2_decade:.4f}\")\nprint(f\"   Degrees of freedom: {dof_decade}\")\nprint(f\"   P-value: {p_decade:.6f}\")\nprint(f\"   Result: {'REJECT H0' if p_decade < 0.05 else 'FAIL TO REJECT H0'} (α=0.05)\")\n\nif p_decade < 0.05:\n    print(\"   → Bin distributions have significantly changed over decades\")\nelse:\n    print(\"   → Bin distributions are consistent across decades\")\n\nprint(\"\\n\" + \"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Summary statistics for binned data\nprint(\"=\"*80)\nprint(\"SUMMARY: BINNED ANALYSIS KEY FINDINGS\")\nprint(\"=\"*80)\n\nprint(\"\\n1. OVERALL BIN DISTRIBUTION:\")\nbin_counts_sorted = df['OnsetBin'].value_counts().reindex(bin_labels)\nmost_common_bin = bin_counts_sorted.idxmax()\nleast_common_bin = bin_counts_sorted.idxmin()\nprint(f\"   • Most common bin: {most_common_bin.split(chr(10))[0]} ({bin_counts_sorted.max()} occurrences)\")\nprint(f\"   • Least common bin: {least_common_bin.split(chr(10))[0]} ({bin_counts_sorted.min()} occurrences)\")\nprint(f\"   • Modal bin percentage: {bin_counts_sorted.max() / len(df) * 100:.1f}%\")\n\nprint(\"\\n2. DECADE PATTERNS:\")\n# Find which bin is most common in each decade\nfor decade_label in decade_labels_list:\n    decade_row = decade_bin_crosstab.loc[decade_label]\n    most_common = decade_row.idxmax()\n    count = decade_row.max()\n    percentage = count / decade_row.sum() * 100\n    print(f\"   • {decade_label}: {most_common.split(chr(10))[0]} ({count} samples, {percentage:.1f}%)\")\n\nprint(\"\\n3. DATA SPLIT PATTERNS:\")\nfor split in split_order:\n    split_row = split_bin_crosstab.loc[split]\n    most_common = split_row.idxmax()\n    count = split_row.max()\n    percentage = count / split_row.sum() * 100\n    print(f\"   • {split:12s}: {most_common.split(chr(10))[0]} ({count} samples, {percentage:.1f}%)\")\n\nprint(\"\\n4. DISTRIBUTION BALANCE:\")\n# Calculate entropy or evenness of distribution\noverall_pct = df['OnsetBin'].value_counts(normalize=True)\nentropy = -np.sum(overall_pct * np.log(overall_pct + 1e-10))\nmax_entropy = np.log(len(bin_labels))\nevenness = entropy / max_entropy\nprint(f\"   • Distribution evenness: {evenness:.3f} (1.0 = perfectly even)\")\nprint(f\"   • Shannon entropy: {entropy:.3f} (max: {max_entropy:.3f})\")\n\nprint(\"\\n5. STATISTICAL SIGNIFICANCE:\")\nprint(f\"   • Split-wise difference: {'Significant' if p_split < 0.05 else 'Not significant'} (p={p_split:.4f})\")\nprint(f\"   • Decade-wise change: {'Significant' if p_decade < 0.05 else 'Not significant'} (p={p_decade:.4f})\")\n\nprint(\"\\n6. BIN STATISTICS:\")\nfor bin_label in bin_labels:\n    bin_data = df[df['OnsetBin'] == bin_label]['DateRelJun01']\n    if len(bin_data) > 0:\n        print(f\"   • {bin_label.split(chr(10))[0]:20s}: n={len(bin_data):3d}, \"\n              f\"mean={bin_data.mean():6.2f}d, std={bin_data.std():5.2f}d\")\n\nprint(\"\\n\" + \"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}